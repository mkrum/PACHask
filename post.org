
#+TITLE: PAC Learning By Example In Haskell
#+OPTIONS: toc:nil author:nil timestamp:nil 

[[https://cs.nyu.edu/~mohri/mlbook/][Link to the book]]

* Probably Approximately Correct (PAC) Learning

What can be learned? Are some things harder to learn than others? 

PAC learning is a powerful framework for thinking about machine learning and all
of these questions. It also, fun fact, is the namesake for this website. The goal of this
installment is to walk through some of the basic ideas of PAC learning by
demonstration in Haskell.

This post heavily relies on the first chapter of the book "Foundations of
Machine Learning" and notes from a series of lectures given by Leslie Valiant.

** Introduction

Machine learning can be roughly described as a process of "learning" a model from
a dataset. Let's make this a little more concrete. 

We have an input space, $\mathcal{X}$. This is the underlying type that will
use to represent our data. For example, if $\mathcal{X} = \mathbb{R}^2$, then
our input is going to be two dimensional real numbers. 

We have the output space, $\mathcal{Y}$. This could be a list of classes,
(e.g. $\mathcal{Y} = \{\text{"cat"},  \text{"dog"} \}"$), or an estimated value for
a regression problem (e.g. $\mathcal{Y} = \mathbb{R}$). For this introduction,
we will focus on binary classification problems, $\mathcal{Y} = \{ 0, 1 \}$. 

What we want to learn a function, or concept, that maps from the input space to
the output space. Mathematically, 
\begin{equation}
c: \mathcal{X} \rightarrow \mathcal{Y}
\end{equation}
In haskell, we can make a type synonym, 
#+BEGIN_SRC haskell
type Concept x y = x -> y
#+END_SRC
The concept $c$ will be part of a larger space of concepts
$\mathcal{C}$. For example, 

We will have some dataset, $S = ((x_1, y_1), \dots, (x_m, y_m))$. Each $x_i \sim
\mathcal{D}$, where $\mathcal{D}$ is some distribution. Each label will be
generated by some hidden conco


